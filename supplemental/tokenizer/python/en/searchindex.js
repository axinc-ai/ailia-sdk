Search.setIndex({"docnames": ["ailia_tokenizer", "index"], "filenames": ["ailia_tokenizer.rst", "index.rst"], "titles": ["ailia_tokenizer package", "ailia Tokenizer Python API document"], "terms": {"basetoken": 0, "base": 0, "object": 0, "convert_tokens_to_id": 0, "token": 0, "decod": 0, "input_id": 0, "skip_special_token": 0, "fals": 0, "encod": 0, "text": 0, "pad": 0, "true": 0, "truncat": 0, "return_tensor": 0, "none": 0, "max_length": 0, "whispertoken": 0, "from_pretrain": 0, "cliptoken": 0, "xlmrobertatoken": 0, "mariantoken": 0, "bertjapanesewordpiecetoken": 0, "vocab_path": 0, "bertjapanesecharactertoken": 0, "t5token": 0, "robertatoken": 0, "marges_path": 0, "bertuncasedtoken": 0, "bertcasedtoken": 0, "ailia_token": 1, "index": 1, "search": 1, "page": 1}, "objects": {"ailia_tokenizer": [[0, 0, 1, "", "BaseTokenizer"], [0, 0, 1, "", "BertCasedTokenizer"], [0, 0, 1, "", "BertJapaneseCharacterTokenizer"], [0, 0, 1, "", "BertJapaneseWordPieceTokenizer"], [0, 0, 1, "", "BertUncasedTokenizer"], [0, 0, 1, "", "CLIPTokenizer"], [0, 0, 1, "", "MarianTokenizer"], [0, 0, 1, "", "RobertaTokenizer"], [0, 0, 1, "", "T5Tokenizer"], [0, 0, 1, "", "WhisperTokenizer"], [0, 0, 1, "", "XLMRobertaTokenizer"]], "ailia_tokenizer.BaseTokenizer": [[0, 1, 1, "", "convert_tokens_to_ids"], [0, 1, 1, "", "decode"], [0, 1, 1, "", "encode"]], "ailia_tokenizer.BertCasedTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.BertJapaneseCharacterTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.BertJapaneseWordPieceTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.BertUncasedTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.CLIPTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.MarianTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.RobertaTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.T5Tokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.WhisperTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.XLMRobertaTokenizer": [[0, 1, 1, "", "from_pretrained"]]}, "objtypes": {"0": "py:class", "1": "py:method"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"]}, "titleterms": {"ailia_token": 0, "packag": [0, 1], "class": 0, "ailia": 1, "token": 1, "python": 1, "api": 1, "document": 1, "indic": 1, "tabl": 1}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 58}, "alltitles": {"ailia_tokenizer package": [[0, "ailia-tokenizer-package"]], "Classes": [[0, "classes"]], "ailia Tokenizer Python API document": [[1, "ailia-tokenizer-python-api-document"]], "Packages": [[1, "packages"]], "Indices and tables": [[1, "indices-and-tables"]]}, "indexentries": {"basetokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BaseTokenizer"]], "bertcasedtokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BertCasedTokenizer"]], "bertjapanesecharactertokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BertJapaneseCharacterTokenizer"]], "bertjapanesewordpiecetokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BertJapaneseWordPieceTokenizer"]], "bertuncasedtokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BertUncasedTokenizer"]], "cliptokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.CLIPTokenizer"]], "mariantokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.MarianTokenizer"]], "robertatokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.RobertaTokenizer"]], "t5tokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.T5Tokenizer"]], "whispertokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.WhisperTokenizer"]], "xlmrobertatokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.XLMRobertaTokenizer"]], "convert_tokens_to_ids() (ailia_tokenizer.basetokenizer method)": [[0, "ailia_tokenizer.BaseTokenizer.convert_tokens_to_ids"]], "decode() (ailia_tokenizer.basetokenizer method)": [[0, "ailia_tokenizer.BaseTokenizer.decode"]], "encode() (ailia_tokenizer.basetokenizer method)": [[0, "ailia_tokenizer.BaseTokenizer.encode"]], "from_pretrained() (ailia_tokenizer.bertcasedtokenizer method)": [[0, "ailia_tokenizer.BertCasedTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.bertjapanesecharactertokenizer method)": [[0, "ailia_tokenizer.BertJapaneseCharacterTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.bertjapanesewordpiecetokenizer method)": [[0, "ailia_tokenizer.BertJapaneseWordPieceTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.bertuncasedtokenizer method)": [[0, "ailia_tokenizer.BertUncasedTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.cliptokenizer method)": [[0, "ailia_tokenizer.CLIPTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.mariantokenizer method)": [[0, "ailia_tokenizer.MarianTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.robertatokenizer method)": [[0, "ailia_tokenizer.RobertaTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.t5tokenizer method)": [[0, "ailia_tokenizer.T5Tokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.whispertokenizer method)": [[0, "ailia_tokenizer.WhisperTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.xlmrobertatokenizer method)": [[0, "ailia_tokenizer.XLMRobertaTokenizer.from_pretrained"]]}})