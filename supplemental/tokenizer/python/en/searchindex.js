Search.setIndex({"alltitles": {"Classes": [[0, "classes"]], "Indices and tables": [[1, "indices-and-tables"]], "Packages": [[1, "packages"]], "ailia Tokenizer Python API document": [[1, null]], "ailia_tokenizer package": [[0, null]]}, "docnames": ["ailia_tokenizer", "index"], "envversion": {"sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2}, "filenames": ["ailia_tokenizer.rst", "index.rst"], "indexentries": {"add_special_tokens() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.add_special_tokens", false]], "ailiatokenizerresult (class in ailia_tokenizer)": [[0, "ailia_tokenizer.AiliaTokenizerResult", false]], "batch_decode() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.batch_decode", false]], "batch_encode_plus() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.batch_encode_plus", false]], "bertjapanesecharactertokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BertJapaneseCharacterTokenizer", false]], "bertjapanesewordpiecetokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BertJapaneseWordPieceTokenizer", false]], "berttokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BertTokenizer", false]], "char_to_token() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.char_to_token", false]], "cliptokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.CLIPTokenizer", false]], "convert_ids_to_tokens() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.convert_ids_to_tokens", false]], "convert_tokens_to_ids() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.convert_tokens_to_ids", false]], "decode() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.decode", false]], "encode() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.encode", false]], "encode_plus() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.encode_plus", false]], "from_pretrained() (ailia_tokenizer.bertjapanesecharactertokenizer class method)": [[0, "ailia_tokenizer.BertJapaneseCharacterTokenizer.from_pretrained", false]], "from_pretrained() (ailia_tokenizer.bertjapanesewordpiecetokenizer class method)": [[0, "ailia_tokenizer.BertJapaneseWordPieceTokenizer.from_pretrained", false]], "from_pretrained() (ailia_tokenizer.berttokenizer class method)": [[0, "ailia_tokenizer.BertTokenizer.from_pretrained", false]], "from_pretrained() (ailia_tokenizer.cliptokenizer class method)": [[0, "ailia_tokenizer.CLIPTokenizer.from_pretrained", false]], "from_pretrained() (ailia_tokenizer.gpt2tokenizer class method)": [[0, "ailia_tokenizer.GPT2Tokenizer.from_pretrained", false]], "from_pretrained() (ailia_tokenizer.llamatokenizer class method)": [[0, "ailia_tokenizer.LlamaTokenizer.from_pretrained", false]], "from_pretrained() (ailia_tokenizer.mariantokenizer class method)": [[0, "ailia_tokenizer.MarianTokenizer.from_pretrained", false]], "from_pretrained() (ailia_tokenizer.robertatokenizer class method)": [[0, "ailia_tokenizer.RobertaTokenizer.from_pretrained", false]], "from_pretrained() (ailia_tokenizer.t5tokenizer class method)": [[0, "ailia_tokenizer.T5Tokenizer.from_pretrained", false]], "from_pretrained() (ailia_tokenizer.whispertokenizer class method)": [[0, "ailia_tokenizer.WhisperTokenizer.from_pretrained", false]], "from_pretrained() (ailia_tokenizer.xlmrobertatokenizer class method)": [[0, "ailia_tokenizer.XLMRobertaTokenizer.from_pretrained", false]], "gpt2tokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.GPT2Tokenizer", false]], "items() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.items", false]], "keys() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.keys", false]], "llamatokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.LlamaTokenizer", false]], "mariantokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.MarianTokenizer", false]], "pretrainedtokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.PreTrainedTokenizer", false]], "robertatokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.RobertaTokenizer", false]], "sequence_ids() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.sequence_ids", false]], "t5tokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.T5Tokenizer", false]], "token_to_word() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.token_to_word", false]], "tokenize() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.tokenize", false]], "whispertokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.WhisperTokenizer", false]], "word_ids() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.word_ids", false]], "word_to_chars() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.word_to_chars", false]], "xlmrobertatokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.XLMRobertaTokenizer", false]]}, "objects": {"ailia_tokenizer": [[0, 0, 1, "", "AiliaTokenizerResult"], [0, 0, 1, "", "BertJapaneseCharacterTokenizer"], [0, 0, 1, "", "BertJapaneseWordPieceTokenizer"], [0, 0, 1, "", "BertTokenizer"], [0, 0, 1, "", "CLIPTokenizer"], [0, 0, 1, "", "GPT2Tokenizer"], [0, 0, 1, "", "LlamaTokenizer"], [0, 0, 1, "", "MarianTokenizer"], [0, 0, 1, "", "PreTrainedTokenizer"], [0, 0, 1, "", "RobertaTokenizer"], [0, 0, 1, "", "T5Tokenizer"], [0, 0, 1, "", "WhisperTokenizer"], [0, 0, 1, "", "XLMRobertaTokenizer"]], "ailia_tokenizer.AiliaTokenizerResult": [[0, 1, 1, "", "char_to_token"], [0, 1, 1, "", "items"], [0, 1, 1, "", "keys"], [0, 1, 1, "", "sequence_ids"], [0, 1, 1, "", "token_to_word"], [0, 1, 1, "", "word_ids"], [0, 1, 1, "", "word_to_chars"]], "ailia_tokenizer.BertJapaneseCharacterTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.BertJapaneseWordPieceTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.BertTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.CLIPTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.GPT2Tokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.LlamaTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.MarianTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.PreTrainedTokenizer": [[0, 1, 1, "", "add_special_tokens"], [0, 1, 1, "", "batch_decode"], [0, 1, 1, "", "batch_encode_plus"], [0, 1, 1, "", "convert_ids_to_tokens"], [0, 1, 1, "", "convert_tokens_to_ids"], [0, 1, 1, "", "decode"], [0, 1, 1, "", "encode"], [0, 1, 1, "", "encode_plus"], [0, 1, 1, "", "tokenize"]], "ailia_tokenizer.RobertaTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.T5Tokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.WhisperTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.XLMRobertaTokenizer": [[0, 1, 1, "", "from_pretrained"]]}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"]}, "objtypes": {"0": "py:class", "1": "py:method"}, "terms": {"": 0, "0": 0, "1": 0, "101": 0, "102": 0, "1037": 0, "1110": 0, "1188": 0, "119": 0, "12": 0, "17": 0, "170": 0, "2": 0, "2003": 0, "2023": 0, "2774": 0, "3": 0, "3231": 0, "49407": 0, "50256": 0, "50257": 0, "6": 0, "7592": 0, "8": 0, "A": 0, "If": 0, "The": 0, "To": 0, "_char_end": 0, "_char_start": 0, "_retain_sot_replace_to_eot": 0, "_sequence_id": 0, "_word_id": 0, "add": 0, "add_special_token": 0, "added_token": 0, "addit": 0, "addition": 0, "additional_special_token": 0, "ailia": 0, "ailia_token": 1, "ailiatokenizererror": 0, "ailiatokenizerresult": 0, "ailiatokenizerresultwithtokentypeid": 0, "allow": 0, "an": 0, "anoth": 0, "api": 0, "appli": 0, "applic": 0, "ar": 0, "architectur": 0, "argument": 0, "arrai": 0, "associ": 0, "attention_mask": 0, "attribut": 0, "automat": 0, "avail": 0, "backend": 0, "base": 0, "batch": 0, "batch_decod": 0, "batch_encode_plu": 0, "batch_index": 0, "batch_or_char_index": 0, "batch_or_token_index": 0, "batch_or_word_index": 0, "beauti": 0, "been": 0, "behavior": 0, "bert": 0, "bertjapanesecharactertoken": 0, "bertjapanesewordpiecetoken": 0, "berttoken": 0, "bool": 0, "both": 0, "boundari": 0, "bpe": 0, "brown": 0, "char_end": 0, "char_index": 0, "char_start": 0, "char_to_token": 0, "charact": 0, "charspan": 0, "cl": 0, "classmethod": 0, "clip": 0, "cliptoken": 0, "common": 0, "compat": 0, "concaten": 0, "configur": 0, "contain": 0, "convert": 0, "convert_ids_to_token": 0, "convert_tokens_to_id": 0, "correspond": 0, "decod": 0, "default": 0, "describ": 0, "determin": 0, "dict": 0, "dict_item": 0, "dict_kei": 0, "dict_path": 0, "dictionari": 0, "directori": 0, "do_not_pad": 0, "doe": 0, "dure": 0, "dynam": 0, "e": 0, "each": 0, "element": 0, "enabl": 0, "encod": 0, "encode_plu": 0, "encodingfast": 0, "end": 0, "english": 0, "eo": 0, "eot": 0, "equival": 0, "etc": 0, "exampl": 0, "exce": 0, "exclud": 0, "exist": 0, "expect": 0, "explicitli": 0, "extern": 0, "extra_id_0": 0, "extra_id_1": 0, "fail": 0, "fairseq": 0, "fals": 0, "field": 0, "file": 0, "first": 0, "fix": 0, "folder": 0, "format": 0, "found": 0, "fox": 0, "from": 0, "from_pretrain": 0, "fulli": 0, "function": 0, "g": 0, "gener": 0, "german": 0, "given": 0, "gpt": 0, "gpt2token": 0, "group": 0, "ha": 0, "hello": 0, "huggingfac": 0, "i": 0, "id": 0, "implement": 0, "includ": 0, "index": [0, 1], "indic": 0, "initi": 0, "input": 0, "input_id": 0, "instanc": 0, "int": 0, "integ": 0, "intern": 0, "interpret": 0, "invalid": 0, "ipad": 0, "item": 0, "iter": 0, "japanes": 0, "json": 0, "kei": 0, "length": 0, "level": 0, "like": 0, "list": 0, "llama": 0, "llamatoken": 0, "load": 0, "local": 0, "longest": 0, "longest_first": 0, "machin": 0, "mai": 0, "map": 0, "marian": 0, "mariantoken": 0, "marker": 0, "max_length": 0, "maximum": 0, "mecab": 0, "merg": 0, "mode": 0, "model": 0, "multilingu": 0, "multipl": 0, "name": 0, "namedtupl": 0, "ndarrai": 0, "nice": 0, "non": 0, "none": 0, "note": 0, "np": 0, "numpi": 0, "object": 0, "offset": 0, "omit": 0, "one": 0, "onli": 0, "only_first": 0, "only_second": 0, "openai": 0, "option": 0, "otherwis": 0, "out": 0, "output": 0, "pad": 0, "pad_token": 0, "page": 1, "pair": 0, "paramet": 0, "patch32": 0, "path": 0, "posit": 0, "prefer": 0, "preprocess": 0, "pretrain": 0, "pretrained_model_name_or_path": 0, "pretrainedtoken": 0, "proper": 0, "properli": 0, "provid": 0, "python": 0, "quick": 0, "rais": 0, "rang": 0, "re": 0, "readi": 0, "remov": 0, "replac": 0, "repres": 0, "requir": 0, "result": 0, "retain": 0, "retriev": 0, "return": 0, "return_tensor": 0, "return_token_type_id": 0, "roberta": 0, "robertatoken": 0, "search": 1, "second": 0, "sentenc": 0, "sentencepiec": 0, "sep": 0, "separ": 0, "seq2seq": 0, "sequenc": 0, "sequence_id": 0, "sequence_index": 0, "set": 0, "shape": 0, "singl": 0, "skip_special_token": 0, "small": 0, "some": 0, "sot": 0, "sourc": 0, "span": 0, "special": 0, "special_tokens_dict": 0, "specif": 0, "specifi": 0, "spiec": 0, "split": 0, "split_special_token": 0, "spm": 0, "start": 0, "str": 0, "strategi": 0, "string": 0, "structur": 0, "style": 0, "subword": 0, "sunshin": 0, "support": 0, "t5": 0, "t5token": 0, "tensor": 0, "test": 0, "text": 0, "text_pair": 0, "thi": 0, "todai": 0, "tok": 0, "token": 0, "token_index": 0, "token_to_word": 0, "token_type_id": 0, "tokenizer_config": 0, "transform": 0, "translat": 0, "treat": 0, "true": 0, "truncat": 0, "txt": 0, "type": 0, "uncas": 0, "unsupport": 0, "us": 0, "utf": 0, "valu": 0, "variant": 0, "variou": 0, "view": 0, "vit": 0, "vocab": 0, "vocabulari": 0, "weather": 0, "when": 0, "whether": 0, "which": 0, "whisper": 0, "whispertoken": 0, "within": 0, "word": 0, "word_id": 0, "word_index": 0, "word_to_char": 0, "wordpiec": 0, "world": 0, "xlm": 0, "xlmrobertatoken": 0, "yet": 0, "\u3057\u307e\u3059": 0, "\u30c6\u30ad\u30b9\u30c8": 0, "\u5206": 0, "\u65e5\u672c": 0, "\u65e5\u672c\u8a9e\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u5206\u304b\u3061\u66f8\u304d\u3057\u307e\u3059": 0, "\u66f8": 0, "\u8a9e": 0}, "titles": ["ailia_tokenizer package", "ailia Tokenizer Python API document"], "titleterms": {"ailia": 1, "ailia_token": 0, "api": 1, "class": 0, "document": 1, "indic": 1, "packag": [0, 1], "python": 1, "tabl": 1, "token": 1}})