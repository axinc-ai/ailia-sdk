Search.setIndex({"docnames": ["ailia_tokenizer", "index"], "filenames": ["ailia_tokenizer.rst", "index.rst"], "titles": ["ailia_tokenizer package", "ailia Tokenizer Python API document"], "terms": {"ailiatokenizerresult": 0, "input_id": 0, "attention_mask": 0, "sequence_id": 0, "word_id": 0, "char_start": 0, "char_end": 0, "base": 0, "object": 0, "char_to_token": 0, "batch_or_char_index": 0, "int": 0, "char_index": 0, "none": 0, "item": 0, "kei": 0, "batch": 0, "token_to_word": 0, "batch_or_token_index": 0, "token_index": 0, "batch_index": 0, "0": 0, "word_to_char": 0, "batch_or_word_index": 0, "word_index": 0, "sequence_index": 0, "pretrainedtoken": 0, "add_special_token": 0, "special_tokens_dict": 0, "batch_decod": 0, "sequenc": 0, "list": 0, "skip_special_token": 0, "fals": 0, "str": 0, "batch_encode_plu": 0, "text": 0, "text_pair": 0, "pad": 0, "true": 0, "truncat": 0, "return_tensor": 0, "max_length": 0, "split_special_token": 0, "return_token_type_id": 0, "convert_ids_to_token": 0, "id": 0, "convert_tokens_to_id": 0, "token": 0, "decod": 0, "encod": 0, "encode_plu": 0, "whispertoken": 0, "classmethod": 0, "from_pretrain": 0, "pretrained_model_name_or_path": 0, "cliptoken": 0, "xlmrobertatoken": 0, "mariantoken": 0, "bertjapanesewordpiecetoken": 0, "dict_path": 0, "bertjapanesecharactertoken": 0, "t5token": 0, "robertatoken": 0, "berttoken": 0, "gpt2token": 0, "llamatoken": 0, "ailia_token": 1, "index": 1, "search": 1, "page": 1}, "objects": {"ailia_tokenizer": [[0, 0, 1, "", "AiliaTokenizerResult"], [0, 0, 1, "", "BertJapaneseCharacterTokenizer"], [0, 0, 1, "", "BertJapaneseWordPieceTokenizer"], [0, 0, 1, "", "BertTokenizer"], [0, 0, 1, "", "CLIPTokenizer"], [0, 0, 1, "", "GPT2Tokenizer"], [0, 0, 1, "", "LlamaTokenizer"], [0, 0, 1, "", "MarianTokenizer"], [0, 0, 1, "", "PreTrainedTokenizer"], [0, 0, 1, "", "RobertaTokenizer"], [0, 0, 1, "", "T5Tokenizer"], [0, 0, 1, "", "WhisperTokenizer"], [0, 0, 1, "", "XLMRobertaTokenizer"]], "ailia_tokenizer.AiliaTokenizerResult": [[0, 1, 1, "", "char_to_token"], [0, 1, 1, "", "items"], [0, 1, 1, "", "keys"], [0, 1, 1, "", "sequence_ids"], [0, 1, 1, "", "token_to_word"], [0, 1, 1, "", "word_ids"], [0, 1, 1, "", "word_to_chars"]], "ailia_tokenizer.BertJapaneseCharacterTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.BertJapaneseWordPieceTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.BertTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.CLIPTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.GPT2Tokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.LlamaTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.MarianTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.PreTrainedTokenizer": [[0, 1, 1, "", "add_special_tokens"], [0, 1, 1, "", "batch_decode"], [0, 1, 1, "", "batch_encode_plus"], [0, 1, 1, "", "convert_ids_to_tokens"], [0, 1, 1, "", "convert_tokens_to_ids"], [0, 1, 1, "", "decode"], [0, 1, 1, "", "encode"], [0, 1, 1, "", "encode_plus"], [0, 1, 1, "", "tokenize"]], "ailia_tokenizer.RobertaTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.T5Tokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.WhisperTokenizer": [[0, 1, 1, "", "from_pretrained"]], "ailia_tokenizer.XLMRobertaTokenizer": [[0, 1, 1, "", "from_pretrained"]]}, "objtypes": {"0": "py:class", "1": "py:method"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"]}, "titleterms": {"ailia_token": 0, "packag": [0, 1], "class": 0, "ailia": 1, "token": 1, "python": 1, "api": 1, "document": 1, "indic": 1, "tabl": 1}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 58}, "alltitles": {"ailia_tokenizer package": [[0, "ailia-tokenizer-package"]], "Classes": [[0, "classes"]], "ailia Tokenizer Python API document": [[1, "ailia-tokenizer-python-api-document"]], "Packages": [[1, "packages"]], "Indices and tables": [[1, "indices-and-tables"]]}, "indexentries": {"ailiatokenizerresult (class in ailia_tokenizer)": [[0, "ailia_tokenizer.AiliaTokenizerResult"]], "bertjapanesecharactertokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BertJapaneseCharacterTokenizer"]], "bertjapanesewordpiecetokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BertJapaneseWordPieceTokenizer"]], "berttokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.BertTokenizer"]], "cliptokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.CLIPTokenizer"]], "gpt2tokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.GPT2Tokenizer"]], "llamatokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.LlamaTokenizer"]], "mariantokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.MarianTokenizer"]], "pretrainedtokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.PreTrainedTokenizer"]], "robertatokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.RobertaTokenizer"]], "t5tokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.T5Tokenizer"]], "whispertokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.WhisperTokenizer"]], "xlmrobertatokenizer (class in ailia_tokenizer)": [[0, "ailia_tokenizer.XLMRobertaTokenizer"]], "add_special_tokens() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.add_special_tokens"]], "batch_decode() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.batch_decode"]], "batch_encode_plus() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.batch_encode_plus"]], "char_to_token() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.char_to_token"]], "convert_ids_to_tokens() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.convert_ids_to_tokens"]], "convert_tokens_to_ids() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.convert_tokens_to_ids"]], "decode() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.decode"]], "encode() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.encode"]], "encode_plus() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.encode_plus"]], "from_pretrained() (ailia_tokenizer.bertjapanesecharactertokenizer class method)": [[0, "ailia_tokenizer.BertJapaneseCharacterTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.bertjapanesewordpiecetokenizer class method)": [[0, "ailia_tokenizer.BertJapaneseWordPieceTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.berttokenizer class method)": [[0, "ailia_tokenizer.BertTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.cliptokenizer class method)": [[0, "ailia_tokenizer.CLIPTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.gpt2tokenizer class method)": [[0, "ailia_tokenizer.GPT2Tokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.llamatokenizer class method)": [[0, "ailia_tokenizer.LlamaTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.mariantokenizer class method)": [[0, "ailia_tokenizer.MarianTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.robertatokenizer class method)": [[0, "ailia_tokenizer.RobertaTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.t5tokenizer class method)": [[0, "ailia_tokenizer.T5Tokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.whispertokenizer class method)": [[0, "ailia_tokenizer.WhisperTokenizer.from_pretrained"]], "from_pretrained() (ailia_tokenizer.xlmrobertatokenizer class method)": [[0, "ailia_tokenizer.XLMRobertaTokenizer.from_pretrained"]], "items() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.items"]], "keys() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.keys"]], "sequence_ids() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.sequence_ids"]], "token_to_word() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.token_to_word"]], "tokenize() (ailia_tokenizer.pretrainedtokenizer method)": [[0, "ailia_tokenizer.PreTrainedTokenizer.tokenize"]], "word_ids() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.word_ids"]], "word_to_chars() (ailia_tokenizer.ailiatokenizerresult method)": [[0, "ailia_tokenizer.AiliaTokenizerResult.word_to_chars"]]}})