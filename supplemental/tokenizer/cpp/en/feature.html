<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ailia_tokenizer: Features</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ailia_tokenizer
   &#160;<span id="projectnumber">1.5.0.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Features </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="autotoc_md57"></a>
Features of ailia Tokenizer</h1>
<p>In this page, we present the features that are provided by both the C and the C# APIs.</p>
<h2><a class="anchor" id="autotoc_md58"></a>
Compatibility of Tokenizer</h2>
<p>ailiaTokenizerEncode matches the following call from transformers. SpecialToken will be encoded as text. No padding or truncation occurs.</p>
<div class="fragment"><div class="line">input_ids = tokenizer(sents, split_special_tokens=True)</div>
</div><!-- fragment --><p>ailiaTokenizerEncodeWithSpecialTokens matches the following call from transformers. SpecialToken will be encoded as SpecialToken. No padding or truncation occurs.</p>
<div class="fragment"><div class="line">input_ids = tokenizer(sents)</div>
</div><!-- fragment --><p>ailiaTokenizerDecode matches the following call from transformers. SpecialToken will not be output.</p>
<div class="fragment"><div class="line">tokenizer.decode(input_ids, skip_special_tokens=True)</div>
</div><!-- fragment --><p>ailiaTokenizerDecodeWithSpecialTokens matches the following call from transformers. SpecialToken will be output.</p>
<div class="fragment"><div class="line">tokenizer.decode(input_ids)</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md59"></a>
Tokenizer Types</h2>
<h3><a class="anchor" id="autotoc_md60"></a>
AILIA_TOKENIZER_TYPE_WHISPER</h3>
<p>The following Python processes are supported.</p>
<div class="fragment"><div class="line">from transformers import WhisperTokenizer</div>
<div class="line">tokenizer = WhisperTokenizer.from_pretrained(&quot;openai/whisper-base&quot;, predict_timestamps=True)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><p>If you want to match the following implementation of OpenAI, please remove the leading SOT and the trailing EOT.</p>
<div class="fragment"><div class="line">from tokenizer import get_tokenizer</div>
<div class="line">is_multilingual = True</div>
<div class="line">tokenizer = get_tokenizer(is_multilingual)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md61"></a>
AILIA_TOKENIZER_TYPE_CLIP</h3>
<p>Corresponds to the following Python process; SOT and EOT will be given. Padding is not performed, so if necessary, pad trailing zeros up to 77 symbols.</p>
<div class="fragment"><div class="line">from transformers import CLIPTokenizer</div>
<div class="line">tokenizer = CLIPTokenizer.from_pretrained(&quot;openai/clip-vit-large-patch14&quot;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><div class="fragment"><div class="line">from simple_tokenizer import SimpleTokenizer as _Tokenizer</div>
<div class="line">_tokenizer = _Tokenizer()</div>
<div class="line">sot_token = _tokenizer.encoder[&quot;&lt;|startoftext|&gt;&quot;]</div>
<div class="line">eot_token = _tokenizer.encoder[&quot;&lt;|endoftext|&gt;&quot;]</div>
<div class="line">all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md62"></a>
AILIA_TOKENIZER_TYPE_XLM_ROBERTA</h3>
<p>Corresponds to the following process in Python. Separately, you will need to provide the sentimentpiece.bpe.model to the ailiaTokenizerOpenModelFile.</p>
<div class="fragment"><div class="line">from transformers import XLMRobertaTokenizer</div>
<div class="line">tokenizer = XLMRobertaTokenizer.from_pretrained(&#39;sentence-transformers/paraphrase-multilingual-mpnet-base-v2&#39;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md63"></a>
AILIA_TOKENIZER_TYPE_MARIAN</h3>
<p>Corresponds to the following process in Python. Separately, source.spm must be given to ailiaTokenizerOpenModelFile.</p>
<div class="fragment"><div class="line">from transformers import MarianTokenizer</div>
<div class="line">tokenizer = MarianTokenizer.from_pretrained(&quot;staka/fugumt-en-ja&quot;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md64"></a>
AILIA_TOKENIZER_TYPE_BERT_JAPANESE_WORDPIECE</h3>
<p>The following Python processes are supported: UKFC conversion is done by ailia Tokenizer. You need to give ipadic and tokenizer_wordpiece/vocab.txt separately.</p>
<div class="fragment"><div class="line">from transformers import BertJapaneseTokenizer</div>
<div class="line">tokenizer = BertJapaneseTokenizer.from_pretrained(&#39;cl-tohoku/bert-base-japanese-whole-word-masking&#39;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><p>If you want to match with convert_tokens_to_ids, you need to remove the [CLS] and [SEP] symbols from the beginning and the end.</p>
<div class="fragment"><div class="line">tokenized_text = tokenizer.tokenize(text)</div>
<div class="line">indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md65"></a>
AILIA_TOKENIZER_TYPE_BERT_JAPANESE_CHARACTER</h3>
<p>The following Python processes are supported: UKFC conversion is done by ailia Tokenizer. Separately, ipadic must be given to ailiaTokenizerOpenDictionaryFile and tokenizer_character/vocab.txt to ailiaTokenizerOpenVocabFile.</p>
<div class="fragment"><div class="line">from transformers import BertJapaneseTokenizer</div>
<div class="line">tokenizer = BertJapaneseTokenizer.from_pretrained(&#39;cl-tohoku/bert-base-japanese-char-whole-word-maskin&#39;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><p>If you want to match with convert_tokens_to_ids, you need to remove the [CLS] and [SEP] symbols from the beginning and the end.</p>
<div class="fragment"><div class="line">tokenized_text = tokenizer.tokenize(text)</div>
<div class="line">indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md66"></a>
AILIA_TOKENIZER_TYPE_T5</h3>
<p>Corresponds to the following Python process, which inserts an EOS symbol at the end of the output in the same way as add_special_tokens=True. Remove the EOS symbol at the end of the output. Separately, you need to give the file "spiece.model" to AsiaTokenizerOpenModelFile.</p>
<div class="fragment"><div class="line">from transformers import T5Tokenizer</div>
<div class="line">tokenizer = T5Tokenizer.from_pretrained(&#39;sonoisa/t5-base-japanese-title-generation&#39;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md67"></a>
AILIA_TOKENIZER_TYPE_ROBERTA</h3>
<p>Corresponds to the following Python process, which inserts an EOS symbol at the end of the output. Separately, you need to provide vocab.json to ailiaTokenizerOpenVocabFile and merges.txt to ailiaTokenizerOpenMergeFile.</p>
<div class="fragment"><div class="line">from transformers import RobertaTokenizer</div>
<div class="line">tokenize = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md68"></a>
AILIA_TOKENIZER_TYPE_BERT</h3>
<p>Corresponds to the following Python process. You need to give vocab.txt and tokenizer_config.json separately.</p>
<div class="fragment"><div class="line">from transformers import BertTokenizer</div>
<div class="line">tokenizer = BertTokenizer.from_pretrained(&#39;google-bert/bert-base-uncased&#39;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><div class="fragment"><div class="line">from transformers import BertTokenizer</div>
<div class="line">tokenizer = BertTokenizer.from_pretrained(&#39;google-bert/bert-base-cased&#39;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><p>If you want to match with convert_tokens_to_ids, you need to remove the [CLS] and [SEP] symbols from the beginning and the end.</p>
<div class="fragment"><div class="line">tokenized_text = tokenizer.tokenize(text)</div>
<div class="line">indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md69"></a>
AILIA_TOKENIZER_TYPE_GPT2</h3>
<p>Corresponds to the following Python process. Separately, you need to provide vocab.json and merges.txt.</p>
<div class="fragment"><div class="line">from transformers import GPT2Tokenizer</div>
<div class="line">tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md70"></a>
AILIA_TOKENIZER_TYPE_LLAMA</h3>
<p>Corresponds to the following Python process. Separately, tokenizer.model must be given to ailiaTokenizerOpenModelFile.</p>
<div class="fragment"><div class="line">from transformers import LlamaTokenizer</div>
<div class="line">tokenizer = LlamaTokenizer.from_pretrained(&quot;liuhaotian/llava-v1.5-7b&quot;)</div>
<div class="line">inputs = tokenizer(sents)</div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1
</small></address>
</body>
</html>
