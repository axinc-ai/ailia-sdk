<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ailia_tokenizer: Features</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ailia_tokenizer
   &#160;<span id="projectnumber">1.1.2.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Features </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="autotoc_md53"></a>
Features of ailia Tokenizer</h1>
<p>In this page, we present the features that are provided by both the C and the C# APIs.</p>
<h2><a class="anchor" id="autotoc_md54"></a>
Tokenizer Types</h2>
<h3><a class="anchor" id="autotoc_md55"></a>
AILIA_TOKENIZER_TYPE_WHISPER</h3>
<p>The following Python processes are supported.</p>
<div class="fragment"><div class="line">from tokenizer import get_tokenizer</div>
<div class="line">is_multilingual = True</div>
<div class="line">tokenizer = get_tokenizer(is_multilingual)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md56"></a>
AILIA_TOKENIZER_TYPE_CLIP</h3>
<p>Corresponds to the following Python process; SOT and EOT will be given. Padding is not performed, so if necessary, pad trailing zeros up to 77 symbols.</p>
<div class="fragment"><div class="line">from simple_tokenizer import SimpleTokenizer as _Tokenizer</div>
<div class="line">_tokenizer = _Tokenizer()</div>
<div class="line">sot_token = _tokenizer.encoder[&quot;&lt;|startoftext|&gt;&quot;]</div>
<div class="line">eot_token = _tokenizer.encoder[&quot;&lt;|endoftext|&gt;&quot;]</div>
<div class="line">all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md57"></a>
AILIA_TOKENIZER_TYPE_XLM_ROBERTA</h3>
<p>Corresponds to the following process in Python. Separately, you will need to provide the sentimentpiece.bpe.model to the ailiaTokenizerOpenModelFile.</p>
<div class="fragment"><div class="line">from transformers import AutoTokenizer</div>
<div class="line">tokenizer = AutoTokenizer.from_pretrained(&#39;sentence-transformers/paraphrase-multilingual-mpnet-base-v2&#39;)</div>
<div class="line">inputs = tokenizer(sents, padding=True, truncation=True, return_tensors=&#39;np&#39;)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md58"></a>
AILIA_TOKENIZER_TYPE_MARIAN</h3>
<p>Corresponds to the following process in Python. Separately, source.spm must be given to ailiaTokenizerOpenModelFile.</p>
<div class="fragment"><div class="line">from transformers import MarianTokenizer</div>
<div class="line">tokenizer = MarianTokenizer.from_pretrained(&quot;staka/fugumt-en-ja&quot;)</div>
<div class="line">model_inputs = tokenizer(sents)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md59"></a>
AILIA_TOKENIZER_TYPE_BERT_JAPANESE_WORDPIECE</h3>
<p>The following Python processes are supported: UKFC conversion is done by ailia Tokenizer. You need to give ipadic and vocab_wordpiece.txt separately.</p>
<div class="fragment"><div class="line">tokenizer = BertJapaneseTokenizer.from_pretrained(&#39;cl-tohoku/bert-base-japanese-whole-word-masking&#39;)</div>
<div class="line">tokenized_text = tokenizer.tokenize(text)</div>
<div class="line">indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md60"></a>
AILIA_TOKENIZER_TYPE_BERT_JAPANESE_CHARACTER</h3>
<p>The following Python processes are supported: UKFC conversion is done by ailia Tokenizer. Separately, ipadic must be given to ailiaTokenizerOpenDictionaryFile and vocab_character.txt to ailiaTokenizerOpenVocabFile.</p>
<div class="fragment"><div class="line">tokenizer = BertJapaneseTokenizer.from_pretrained(&#39;cl-tohoku/bert-base-japanese-char-whole-word-maskin&#39;)</div>
<div class="line">tokenized_text = tokenizer.tokenize(text)</div>
<div class="line">indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md61"></a>
AILIA_TOKENIZER_TYPE_T5</h3>
<p>Corresponds to the following Python process, which inserts an EOS symbol at the end of the output in the same way as add_special_tokens=True. Remove the EOS symbol at the end of the output. Separately, you need to give the file "spiece.model" to AsiaTokenizerOpenModelFile.</p>
<div class="fragment"><div class="line">from transformers import AutoTokenizer</div>
<div class="line">tokenizer = AutoTokenizer.from_pretrained(&#39;sonoisa/t5-base-japanese-title-generation&#39;)</div>
<div class="line">inputs = tokenizer(sents, padding=True, truncation=True, return_tensors=&#39;np&#39;)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md62"></a>
AILIA_TOKENIZER_TYPE_ROBERTA</h3>
<p>Corresponds to the following Python process, which inserts an EOS symbol at the end of the output. Separately, you need to provide vocab.json to ailiaTokenizerOpenVocabFile and merges.txt to ailiaTokenizerOpenMergeFile.</p>
<div class="fragment"><div class="line">from transformers import RobertaTokenizer</div>
<div class="line"> </div>
<div class="line">tokenize = RobertaTokenizer.from_pretrained(&#39;roberta-base&#39;)</div>
<div class="line">result = tokenize(</div>
<div class="line">    sents,</div>
<div class="line">    padding=True,</div>
<div class="line">    truncation=True,</div>
<div class="line">    return_tensors=&quot;pt&quot;,</div>
<div class="line">)</div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
